{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factor analysis is a linear statistical model. It is used to explain the variance among the observed variable and condense a set of the observed variable into the unobserved variable called factors. Observed variables are modeled as a linear combination of factors and error terms (Source). Factor or latent variable is associated with multiple observed variables, who have common patterns of responses. Each factor explains a particular amount of variance in the observed variables. It helps in data interpretations by reducing the number of variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How does Factor Analysis Work?__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary objective of factor analysis is to reduce the number of observed variables and find unobservable variables. These unobserved variables help the market researcher to conclude the survey. This conversion of the observed variables to unobserved variables can be achieved in two steps:\n",
    "\n",
    "__Factor Extraction__: In this step, the number of factors and approach for extraction selected using variance partitioning methods such as principal components analysis and common factor analysis.\n",
    "Factor Rotation: In this step, rotation tries to convert factors into uncorrelated factors — the main goal of this step to improve the overall interpretability. There are lots of rotation methods that are available such as: Varimax rotation method, Quartimax rotation method, and Promax rotation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>everbev</th>\n",
       "      <th>everherb</th>\n",
       "      <th>evermed</th>\n",
       "      <th>oats</th>\n",
       "      <th>lacttreathome</th>\n",
       "      <th>lacttreatstore</th>\n",
       "      <th>brewersyeast</th>\n",
       "      <th>coconutdrink</th>\n",
       "      <th>sportdrink</th>\n",
       "      <th>...</th>\n",
       "      <th>books</th>\n",
       "      <th>facebook</th>\n",
       "      <th>instagram</th>\n",
       "      <th>twitter</th>\n",
       "      <th>pinterest</th>\n",
       "      <th>onlineforum</th>\n",
       "      <th>blog</th>\n",
       "      <th>websites</th>\n",
       "      <th>searchengine</th>\n",
       "      <th>app</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-99</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  everbev  everherb  evermed  oats  lacttreathome  lacttreatstore  \\\n",
       "0   1        2         2        2     3              3               3   \n",
       "1   2        1         2        2     3              3               1   \n",
       "2   3        1         1        2     2              1               1   \n",
       "3   4        2         2        2     3              3               3   \n",
       "4   5        1         1        2     2              1               1   \n",
       "5   6        2         2        2     3              3               3   \n",
       "6   7        1         2        2     3              2               2   \n",
       "7   8        1         2        2     1            -99             -99   \n",
       "8   9        2         2        2     3              3               3   \n",
       "9  10        2         2        2     3              3               3   \n",
       "\n",
       "   brewersyeast  coconutdrink  sportdrink  ...  books  facebook  instagram  \\\n",
       "0             3             3           3  ...      2         1          2   \n",
       "1             3             3           3  ...      2         2          2   \n",
       "2             1             1           1  ...      2         1          2   \n",
       "3             3             3           3  ...      2         1          2   \n",
       "4             3             2           2  ...      2         1          1   \n",
       "5             3             3           3  ...      2         1          1   \n",
       "6             2             3           1  ...      2         1          2   \n",
       "7           -99           -99           1  ...      2         1          2   \n",
       "8             3             3           3  ...      2         2          2   \n",
       "9             3             3           3  ...      2         1          2   \n",
       "\n",
       "   twitter  pinterest  onlineforum  blog  websites  searchengine  app  \n",
       "0        2          1            2     2         2             2    2  \n",
       "1        2          2            2     2         2             1    2  \n",
       "2        2          2            3     2         1             1    2  \n",
       "3        2          2            2     2         2             2    2  \n",
       "4        1          1            2     2         2             1    2  \n",
       "5        2          2            2     2         1             2    2  \n",
       "6        2          2            1     2         1             2    2  \n",
       "7        2          1          -99     2         2             1    1  \n",
       "8        2          2            2     2         2             2    2  \n",
       "9        2          2            2     2         2             2    2  \n",
       "\n",
       "[10 rows x 65 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionaire_df = pd.read_csv('/Users/soumyadeepray/Documents/Research_Project-Nutrition_Sciences/Galactagogue_buying_patterns/data/Ryan_DataforRay_MissingReplaced_2022.09.30.csv')\n",
    "questionaire_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionaire_df=questionaire_df.replace(-99,np.nan).dropna(axis = 0, how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['everbev', 'everherb', 'evermed', 'oats', 'lacttreathome',\n",
    "       'lacttreatstore', 'brewersyeast', 'coconutdrink', 'sportdrink',\n",
    "       'pinkdrink', 'noalcbeer', 'beer', 'wine', 'lacttea', 'fruit', 'veg',\n",
    "       'seeds', 'beans', 'wholegrains', 'nuts', 'meat',\n",
    "       'increasebf', 'increaseexpress', 'skintoskin', 'shield', 'massage',\n",
    "       'pumpwhilebf', 'pumpboth', 'pumpafter', 'warmed', 'reducestress']\n",
    "questionaire_df = pd.DataFrame(data = questionaire_df, columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adequacy Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing factor analysis, you need to evaluate the “factorability” of our dataset. Factorability means \"can we found the factors in the dataset?\". There are two methods to check the factorability or sampling adequacy:\n",
    "\n",
    "__Bartlett’s Test__\n",
    "\n",
    "__Kaiser-Meyer-Olkin Test__\n",
    "\n",
    "Bartlett’s test of sphericity checks whether or not the observed variables intercorrelate at all using the observed correlation matrix against the identity matrix. If the test found statistically insignificant, you should not employ a factor analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13271.401667764745, 0.0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "chi_square_value,p_value=calculate_bartlett_sphericity(questionaire_df)\n",
    "chi_square_value, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Bartlett ’s test, the p-value is 0. The test was statistically significant, indicating that the observed correlation matrix is not an identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kaiser-Meyer-Olkin (KMO) Test__ measures the suitability of data for factor analysis. It determines the adequacy for each observed variable and for the complete model. KMO estimates the proportion of variance among all the observed variable. Lower proportion id more suitable for factor analysis. KMO values range between 0 and 1. Value of KMO less than 0.6 is considered inadequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/factor_analyzer/utils.py:244: UserWarning: The inverse of the variance-covariance matrix was calculated using the Moore-Penrose generalized matrix inversion, due to its determinant being at or very close to zero.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "kmo_all,kmo_model=calculate_kmo(questionaire_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9118907318997471"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmo_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall KMO for our data is 0.911, which is excellent. This value indicates that we can proceed with our planned factor analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the Number of Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For choosing the number of factors, we can use the Kaiser criterion and scree plot. Both are based on eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.86701041, 2.70652786, 1.98640037, 1.36766219, 1.23165146,\n",
       "       1.16016107, 1.05832909, 0.9228587 , 0.9111215 , 0.87493483,\n",
       "       0.82556004, 0.80511797, 0.7629055 , 0.7349285 , 0.69280871,\n",
       "       0.66332727, 0.62376003, 0.59765189, 0.58303944, 0.56189848,\n",
       "       0.50379676, 0.48817971, 0.46679174, 0.43775335, 0.40756163,\n",
       "       0.36858294, 0.34528238, 0.33193318, 0.28670254, 0.23265077,\n",
       "       0.1931097 ])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create factor analysis object and perform factor analysis\n",
    "fa = FactorAnalyzer()\n",
    "fa.fit(questionaire_df) #(questionaire_df, 25, rotation=None)\n",
    "# Check Eigenvalues\n",
    "ev, v = fa.get_eigenvalues()\n",
    "ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see only for 7-factors eigenvalues are greater than one. It means we need to choose only 6 factors (or unobserved variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create scree plot using matplotlib\n",
    "plt.scatter(range(1,questionaire_df.shape[1]+1),ev)\n",
    "plt.plot(range(1,questionaire_df.shape[1]+1),ev)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Factors')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scree plot method draws a straight line for each factor and its eigenvalues. Number of eigenvalues greater than one is considered as the number of factors.\n",
    "\n",
    "Here, we can see only for 7-factors, eigenvalues are greater than one. It means we need to choose only 7 factors (or unobserved variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Performing Factor Analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>FactorAnalyzer(n_factors=7, rotation=&#x27;varimax&#x27;, rotation_kwargs={})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FactorAnalyzer</label><div class=\"sk-toggleable__content\"><pre>FactorAnalyzer(n_factors=7, rotation=&#x27;varimax&#x27;, rotation_kwargs={})</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "FactorAnalyzer(n_factors=7, rotation='varimax', rotation_kwargs={})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create factor analysis object and perform factor analysis\n",
    "fa = FactorAnalyzer(n_factors=7, rotation=\"varimax\")\n",
    "fa.fit(questionaire_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>everbev</th>\n",
       "      <td>0.198609</td>\n",
       "      <td>0.150189</td>\n",
       "      <td>0.895004</td>\n",
       "      <td>0.208797</td>\n",
       "      <td>0.132124</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>0.022863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everherb</th>\n",
       "      <td>0.118554</td>\n",
       "      <td>0.135638</td>\n",
       "      <td>0.191344</td>\n",
       "      <td>0.230681</td>\n",
       "      <td>0.072322</td>\n",
       "      <td>0.056328</td>\n",
       "      <td>0.275888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evermed</th>\n",
       "      <td>0.069258</td>\n",
       "      <td>0.014133</td>\n",
       "      <td>0.083585</td>\n",
       "      <td>0.046606</td>\n",
       "      <td>0.042753</td>\n",
       "      <td>0.049254</td>\n",
       "      <td>0.382219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oats</th>\n",
       "      <td>0.315879</td>\n",
       "      <td>0.222230</td>\n",
       "      <td>0.577303</td>\n",
       "      <td>0.324995</td>\n",
       "      <td>0.157480</td>\n",
       "      <td>0.142407</td>\n",
       "      <td>-0.002956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lacttreathome</th>\n",
       "      <td>0.164369</td>\n",
       "      <td>0.115254</td>\n",
       "      <td>0.232770</td>\n",
       "      <td>0.645362</td>\n",
       "      <td>0.164560</td>\n",
       "      <td>0.048374</td>\n",
       "      <td>0.022562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lacttreatstore</th>\n",
       "      <td>0.122540</td>\n",
       "      <td>0.171398</td>\n",
       "      <td>0.432104</td>\n",
       "      <td>0.063387</td>\n",
       "      <td>0.198694</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.179635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brewersyeast</th>\n",
       "      <td>0.151742</td>\n",
       "      <td>0.104840</td>\n",
       "      <td>0.135783</td>\n",
       "      <td>0.759115</td>\n",
       "      <td>0.163396</td>\n",
       "      <td>0.026433</td>\n",
       "      <td>0.162711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coconutdrink</th>\n",
       "      <td>0.215432</td>\n",
       "      <td>0.090646</td>\n",
       "      <td>0.189765</td>\n",
       "      <td>0.213184</td>\n",
       "      <td>0.540814</td>\n",
       "      <td>0.122650</td>\n",
       "      <td>0.062053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sportdrink</th>\n",
       "      <td>0.215343</td>\n",
       "      <td>0.214649</td>\n",
       "      <td>0.453019</td>\n",
       "      <td>0.127890</td>\n",
       "      <td>0.400397</td>\n",
       "      <td>0.101493</td>\n",
       "      <td>0.134461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pinkdrink</th>\n",
       "      <td>-0.022256</td>\n",
       "      <td>0.062561</td>\n",
       "      <td>0.126163</td>\n",
       "      <td>0.126436</td>\n",
       "      <td>0.547179</td>\n",
       "      <td>0.026488</td>\n",
       "      <td>0.045078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noalcbeer</th>\n",
       "      <td>0.137367</td>\n",
       "      <td>-0.016331</td>\n",
       "      <td>0.039259</td>\n",
       "      <td>0.039132</td>\n",
       "      <td>0.008076</td>\n",
       "      <td>0.251846</td>\n",
       "      <td>0.124122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beer</th>\n",
       "      <td>0.068396</td>\n",
       "      <td>0.068853</td>\n",
       "      <td>0.102213</td>\n",
       "      <td>0.094925</td>\n",
       "      <td>0.105867</td>\n",
       "      <td>0.623730</td>\n",
       "      <td>0.000479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine</th>\n",
       "      <td>0.174982</td>\n",
       "      <td>0.096004</td>\n",
       "      <td>0.031991</td>\n",
       "      <td>-0.029755</td>\n",
       "      <td>0.009680</td>\n",
       "      <td>0.494588</td>\n",
       "      <td>-0.001272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lacttea</th>\n",
       "      <td>0.161276</td>\n",
       "      <td>0.109217</td>\n",
       "      <td>0.388809</td>\n",
       "      <td>0.091608</td>\n",
       "      <td>0.037381</td>\n",
       "      <td>0.038216</td>\n",
       "      <td>0.087380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fruit</th>\n",
       "      <td>0.757321</td>\n",
       "      <td>0.166602</td>\n",
       "      <td>0.231307</td>\n",
       "      <td>0.070339</td>\n",
       "      <td>0.025464</td>\n",
       "      <td>0.135924</td>\n",
       "      <td>0.042577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veg</th>\n",
       "      <td>0.816034</td>\n",
       "      <td>0.144153</td>\n",
       "      <td>0.173237</td>\n",
       "      <td>0.137699</td>\n",
       "      <td>0.027223</td>\n",
       "      <td>0.080022</td>\n",
       "      <td>0.091348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seeds</th>\n",
       "      <td>0.352960</td>\n",
       "      <td>0.174542</td>\n",
       "      <td>0.141536</td>\n",
       "      <td>0.680194</td>\n",
       "      <td>0.085238</td>\n",
       "      <td>0.073984</td>\n",
       "      <td>0.023206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beans</th>\n",
       "      <td>0.696354</td>\n",
       "      <td>0.143085</td>\n",
       "      <td>0.043313</td>\n",
       "      <td>0.139033</td>\n",
       "      <td>0.048721</td>\n",
       "      <td>0.184251</td>\n",
       "      <td>0.035629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wholegrains</th>\n",
       "      <td>0.618634</td>\n",
       "      <td>0.095418</td>\n",
       "      <td>0.111840</td>\n",
       "      <td>0.154093</td>\n",
       "      <td>0.113057</td>\n",
       "      <td>0.059255</td>\n",
       "      <td>0.160853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nuts</th>\n",
       "      <td>0.688296</td>\n",
       "      <td>0.155783</td>\n",
       "      <td>0.160421</td>\n",
       "      <td>0.255975</td>\n",
       "      <td>0.075643</td>\n",
       "      <td>0.110032</td>\n",
       "      <td>-0.038152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meat</th>\n",
       "      <td>0.758409</td>\n",
       "      <td>0.170109</td>\n",
       "      <td>0.184474</td>\n",
       "      <td>0.053622</td>\n",
       "      <td>0.064631</td>\n",
       "      <td>0.164112</td>\n",
       "      <td>0.074197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>increasebf</th>\n",
       "      <td>0.177653</td>\n",
       "      <td>0.512856</td>\n",
       "      <td>0.126289</td>\n",
       "      <td>0.029350</td>\n",
       "      <td>0.170722</td>\n",
       "      <td>0.035685</td>\n",
       "      <td>-0.100126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>increaseexpress</th>\n",
       "      <td>0.086887</td>\n",
       "      <td>0.668423</td>\n",
       "      <td>0.034654</td>\n",
       "      <td>0.064772</td>\n",
       "      <td>0.026966</td>\n",
       "      <td>0.055471</td>\n",
       "      <td>0.097601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skintoskin</th>\n",
       "      <td>0.179074</td>\n",
       "      <td>0.412754</td>\n",
       "      <td>0.052496</td>\n",
       "      <td>-0.025964</td>\n",
       "      <td>0.209248</td>\n",
       "      <td>0.014568</td>\n",
       "      <td>-0.070177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shield</th>\n",
       "      <td>-0.034852</td>\n",
       "      <td>0.437671</td>\n",
       "      <td>0.098316</td>\n",
       "      <td>0.067765</td>\n",
       "      <td>-0.012897</td>\n",
       "      <td>0.019585</td>\n",
       "      <td>-0.047342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>massage</th>\n",
       "      <td>0.051882</td>\n",
       "      <td>0.679221</td>\n",
       "      <td>0.077856</td>\n",
       "      <td>0.088438</td>\n",
       "      <td>0.019954</td>\n",
       "      <td>0.041641</td>\n",
       "      <td>-0.014083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pumpwhilebf</th>\n",
       "      <td>0.092916</td>\n",
       "      <td>0.463518</td>\n",
       "      <td>0.012645</td>\n",
       "      <td>0.030689</td>\n",
       "      <td>0.034386</td>\n",
       "      <td>0.051899</td>\n",
       "      <td>0.080713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pumpboth</th>\n",
       "      <td>0.028191</td>\n",
       "      <td>0.660034</td>\n",
       "      <td>0.147464</td>\n",
       "      <td>0.077686</td>\n",
       "      <td>-0.065642</td>\n",
       "      <td>0.056883</td>\n",
       "      <td>0.143414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pumpafter</th>\n",
       "      <td>0.114056</td>\n",
       "      <td>0.616640</td>\n",
       "      <td>0.064696</td>\n",
       "      <td>0.029290</td>\n",
       "      <td>0.022007</td>\n",
       "      <td>0.073012</td>\n",
       "      <td>0.149181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warmed</th>\n",
       "      <td>0.090320</td>\n",
       "      <td>0.426228</td>\n",
       "      <td>0.083456</td>\n",
       "      <td>0.126053</td>\n",
       "      <td>0.025764</td>\n",
       "      <td>-0.055726</td>\n",
       "      <td>0.040365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reducestress</th>\n",
       "      <td>0.145829</td>\n",
       "      <td>0.417881</td>\n",
       "      <td>0.067526</td>\n",
       "      <td>0.020851</td>\n",
       "      <td>0.095701</td>\n",
       "      <td>0.030332</td>\n",
       "      <td>-0.094567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0         1         2         3         4         5  \\\n",
       "everbev          0.198609  0.150189  0.895004  0.208797  0.132124  0.146500   \n",
       "everherb         0.118554  0.135638  0.191344  0.230681  0.072322  0.056328   \n",
       "evermed          0.069258  0.014133  0.083585  0.046606  0.042753  0.049254   \n",
       "oats             0.315879  0.222230  0.577303  0.324995  0.157480  0.142407   \n",
       "lacttreathome    0.164369  0.115254  0.232770  0.645362  0.164560  0.048374   \n",
       "lacttreatstore   0.122540  0.171398  0.432104  0.063387  0.198694  0.003521   \n",
       "brewersyeast     0.151742  0.104840  0.135783  0.759115  0.163396  0.026433   \n",
       "coconutdrink     0.215432  0.090646  0.189765  0.213184  0.540814  0.122650   \n",
       "sportdrink       0.215343  0.214649  0.453019  0.127890  0.400397  0.101493   \n",
       "pinkdrink       -0.022256  0.062561  0.126163  0.126436  0.547179  0.026488   \n",
       "noalcbeer        0.137367 -0.016331  0.039259  0.039132  0.008076  0.251846   \n",
       "beer             0.068396  0.068853  0.102213  0.094925  0.105867  0.623730   \n",
       "wine             0.174982  0.096004  0.031991 -0.029755  0.009680  0.494588   \n",
       "lacttea          0.161276  0.109217  0.388809  0.091608  0.037381  0.038216   \n",
       "fruit            0.757321  0.166602  0.231307  0.070339  0.025464  0.135924   \n",
       "veg              0.816034  0.144153  0.173237  0.137699  0.027223  0.080022   \n",
       "seeds            0.352960  0.174542  0.141536  0.680194  0.085238  0.073984   \n",
       "beans            0.696354  0.143085  0.043313  0.139033  0.048721  0.184251   \n",
       "wholegrains      0.618634  0.095418  0.111840  0.154093  0.113057  0.059255   \n",
       "nuts             0.688296  0.155783  0.160421  0.255975  0.075643  0.110032   \n",
       "meat             0.758409  0.170109  0.184474  0.053622  0.064631  0.164112   \n",
       "increasebf       0.177653  0.512856  0.126289  0.029350  0.170722  0.035685   \n",
       "increaseexpress  0.086887  0.668423  0.034654  0.064772  0.026966  0.055471   \n",
       "skintoskin       0.179074  0.412754  0.052496 -0.025964  0.209248  0.014568   \n",
       "shield          -0.034852  0.437671  0.098316  0.067765 -0.012897  0.019585   \n",
       "massage          0.051882  0.679221  0.077856  0.088438  0.019954  0.041641   \n",
       "pumpwhilebf      0.092916  0.463518  0.012645  0.030689  0.034386  0.051899   \n",
       "pumpboth         0.028191  0.660034  0.147464  0.077686 -0.065642  0.056883   \n",
       "pumpafter        0.114056  0.616640  0.064696  0.029290  0.022007  0.073012   \n",
       "warmed           0.090320  0.426228  0.083456  0.126053  0.025764 -0.055726   \n",
       "reducestress     0.145829  0.417881  0.067526  0.020851  0.095701  0.030332   \n",
       "\n",
       "                        6  \n",
       "everbev          0.022863  \n",
       "everherb         0.275888  \n",
       "evermed          0.382219  \n",
       "oats            -0.002956  \n",
       "lacttreathome    0.022562  \n",
       "lacttreatstore   0.179635  \n",
       "brewersyeast     0.162711  \n",
       "coconutdrink     0.062053  \n",
       "sportdrink       0.134461  \n",
       "pinkdrink        0.045078  \n",
       "noalcbeer        0.124122  \n",
       "beer             0.000479  \n",
       "wine            -0.001272  \n",
       "lacttea          0.087380  \n",
       "fruit            0.042577  \n",
       "veg              0.091348  \n",
       "seeds            0.023206  \n",
       "beans            0.035629  \n",
       "wholegrains      0.160853  \n",
       "nuts            -0.038152  \n",
       "meat             0.074197  \n",
       "increasebf      -0.100126  \n",
       "increaseexpress  0.097601  \n",
       "skintoskin      -0.070177  \n",
       "shield          -0.047342  \n",
       "massage         -0.014083  \n",
       "pumpwhilebf      0.080713  \n",
       "pumpboth         0.143414  \n",
       "pumpafter        0.149181  \n",
       "warmed           0.040365  \n",
       "reducestress    -0.094567  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor_df=pd.DataFrame(fa.loadings_,index=questionaire_df.columns)\n",
    "factor_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Factor 0 has high factor loadings for fruit,veg,beans,wholegrains,nuts,meat.\n",
    "\n",
    "* Factor 1 has high factor loadings for increasebf,increaseexpress,massage,pumpboth,pumpafter\n",
    "\n",
    "* Factor 2 has high factor loadings for everbev,oats\n",
    "\n",
    "* Factor 3 has high factor loadings for lacttreathome,brewersyeast,seeds\n",
    "\n",
    "* Factor 4 has high factor loadings for coconutdrink,sportdrink,pinkdrink\n",
    "\n",
    "* Factor 5 has high factor loadings for beer\n",
    "\n",
    "* Factor 6 has none of the high loadings for any variable and is not easily interpretable. Its good if we take only six factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Performing Factor Analysis for 6 factors__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>FactorAnalyzer(n_factors=6, rotation=&#x27;varimax&#x27;, rotation_kwargs={})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FactorAnalyzer</label><div class=\"sk-toggleable__content\"><pre>FactorAnalyzer(n_factors=6, rotation=&#x27;varimax&#x27;, rotation_kwargs={})</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "FactorAnalyzer(n_factors=6, rotation='varimax', rotation_kwargs={})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create factor analysis object and perform factor analysis\n",
    "fa = FactorAnalyzer(n_factors=6, rotation=\"varimax\")\n",
    "fa.fit(questionaire_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>everbev</th>\n",
       "      <td>0.179662</td>\n",
       "      <td>0.145763</td>\n",
       "      <td>0.887842</td>\n",
       "      <td>0.192520</td>\n",
       "      <td>0.103690</td>\n",
       "      <td>0.142331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everherb</th>\n",
       "      <td>0.119083</td>\n",
       "      <td>0.139673</td>\n",
       "      <td>0.226104</td>\n",
       "      <td>0.249323</td>\n",
       "      <td>0.063934</td>\n",
       "      <td>0.062603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evermed</th>\n",
       "      <td>0.075596</td>\n",
       "      <td>0.025610</td>\n",
       "      <td>0.121930</td>\n",
       "      <td>0.082592</td>\n",
       "      <td>0.038293</td>\n",
       "      <td>0.054431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oats</th>\n",
       "      <td>0.298654</td>\n",
       "      <td>0.216706</td>\n",
       "      <td>0.590053</td>\n",
       "      <td>0.310725</td>\n",
       "      <td>0.133938</td>\n",
       "      <td>0.140622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lacttreathome</th>\n",
       "      <td>0.154380</td>\n",
       "      <td>0.109423</td>\n",
       "      <td>0.253263</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.150215</td>\n",
       "      <td>0.049424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lacttreatstore</th>\n",
       "      <td>0.115899</td>\n",
       "      <td>0.172955</td>\n",
       "      <td>0.459063</td>\n",
       "      <td>0.075077</td>\n",
       "      <td>0.177784</td>\n",
       "      <td>0.007863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brewersyeast</th>\n",
       "      <td>0.145918</td>\n",
       "      <td>0.102172</td>\n",
       "      <td>0.172322</td>\n",
       "      <td>0.771507</td>\n",
       "      <td>0.150839</td>\n",
       "      <td>0.031305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coconutdrink</th>\n",
       "      <td>0.208517</td>\n",
       "      <td>0.090722</td>\n",
       "      <td>0.222441</td>\n",
       "      <td>0.218198</td>\n",
       "      <td>0.534585</td>\n",
       "      <td>0.124797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sportdrink</th>\n",
       "      <td>0.204915</td>\n",
       "      <td>0.215248</td>\n",
       "      <td>0.488642</td>\n",
       "      <td>0.134079</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>0.103060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pinkdrink</th>\n",
       "      <td>-0.026280</td>\n",
       "      <td>0.062784</td>\n",
       "      <td>0.151990</td>\n",
       "      <td>0.130657</td>\n",
       "      <td>0.542318</td>\n",
       "      <td>0.025780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noalcbeer</th>\n",
       "      <td>0.135922</td>\n",
       "      <td>-0.012549</td>\n",
       "      <td>0.056632</td>\n",
       "      <td>0.051132</td>\n",
       "      <td>0.006788</td>\n",
       "      <td>0.252401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beer</th>\n",
       "      <td>0.058086</td>\n",
       "      <td>0.068677</td>\n",
       "      <td>0.112334</td>\n",
       "      <td>0.091449</td>\n",
       "      <td>0.100682</td>\n",
       "      <td>0.618432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine</th>\n",
       "      <td>0.167479</td>\n",
       "      <td>0.097125</td>\n",
       "      <td>0.036953</td>\n",
       "      <td>-0.031013</td>\n",
       "      <td>0.008981</td>\n",
       "      <td>0.499941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lacttea</th>\n",
       "      <td>0.153066</td>\n",
       "      <td>0.108205</td>\n",
       "      <td>0.404702</td>\n",
       "      <td>0.091486</td>\n",
       "      <td>0.020758</td>\n",
       "      <td>0.039712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fruit</th>\n",
       "      <td>0.750962</td>\n",
       "      <td>0.167580</td>\n",
       "      <td>0.252348</td>\n",
       "      <td>0.072081</td>\n",
       "      <td>0.015973</td>\n",
       "      <td>0.143569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veg</th>\n",
       "      <td>0.811992</td>\n",
       "      <td>0.146391</td>\n",
       "      <td>0.200592</td>\n",
       "      <td>0.146618</td>\n",
       "      <td>0.019845</td>\n",
       "      <td>0.089903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seeds</th>\n",
       "      <td>0.343782</td>\n",
       "      <td>0.169475</td>\n",
       "      <td>0.163658</td>\n",
       "      <td>0.674930</td>\n",
       "      <td>0.076153</td>\n",
       "      <td>0.077445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beans</th>\n",
       "      <td>0.692657</td>\n",
       "      <td>0.144863</td>\n",
       "      <td>0.062666</td>\n",
       "      <td>0.144969</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>0.193123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wholegrains</th>\n",
       "      <td>0.614104</td>\n",
       "      <td>0.100098</td>\n",
       "      <td>0.146407</td>\n",
       "      <td>0.171936</td>\n",
       "      <td>0.107023</td>\n",
       "      <td>0.070639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nuts</th>\n",
       "      <td>0.677339</td>\n",
       "      <td>0.153917</td>\n",
       "      <td>0.175417</td>\n",
       "      <td>0.250045</td>\n",
       "      <td>0.070032</td>\n",
       "      <td>0.115522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meat</th>\n",
       "      <td>0.753136</td>\n",
       "      <td>0.172618</td>\n",
       "      <td>0.209789</td>\n",
       "      <td>0.060432</td>\n",
       "      <td>0.057661</td>\n",
       "      <td>0.172993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>increasebf</th>\n",
       "      <td>0.169730</td>\n",
       "      <td>0.506908</td>\n",
       "      <td>0.128061</td>\n",
       "      <td>0.022067</td>\n",
       "      <td>0.160797</td>\n",
       "      <td>0.033705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>increaseexpress</th>\n",
       "      <td>0.084712</td>\n",
       "      <td>0.670364</td>\n",
       "      <td>0.050757</td>\n",
       "      <td>0.078129</td>\n",
       "      <td>0.023845</td>\n",
       "      <td>0.057424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skintoskin</th>\n",
       "      <td>0.174477</td>\n",
       "      <td>0.410895</td>\n",
       "      <td>0.057244</td>\n",
       "      <td>-0.029032</td>\n",
       "      <td>0.204508</td>\n",
       "      <td>0.013371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shield</th>\n",
       "      <td>-0.039688</td>\n",
       "      <td>0.434484</td>\n",
       "      <td>0.094826</td>\n",
       "      <td>0.061327</td>\n",
       "      <td>-0.017211</td>\n",
       "      <td>0.015939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>massage</th>\n",
       "      <td>0.046436</td>\n",
       "      <td>0.678201</td>\n",
       "      <td>0.081162</td>\n",
       "      <td>0.088476</td>\n",
       "      <td>0.015768</td>\n",
       "      <td>0.039575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pumpwhilebf</th>\n",
       "      <td>0.091966</td>\n",
       "      <td>0.465718</td>\n",
       "      <td>0.025837</td>\n",
       "      <td>0.041602</td>\n",
       "      <td>0.033764</td>\n",
       "      <td>0.053686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pumpboth</th>\n",
       "      <td>0.025432</td>\n",
       "      <td>0.659752</td>\n",
       "      <td>0.163565</td>\n",
       "      <td>0.091394</td>\n",
       "      <td>-0.070359</td>\n",
       "      <td>0.058227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pumpafter</th>\n",
       "      <td>0.112772</td>\n",
       "      <td>0.617221</td>\n",
       "      <td>0.086803</td>\n",
       "      <td>0.047397</td>\n",
       "      <td>0.018638</td>\n",
       "      <td>0.076224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warmed</th>\n",
       "      <td>0.087590</td>\n",
       "      <td>0.426476</td>\n",
       "      <td>0.093045</td>\n",
       "      <td>0.130618</td>\n",
       "      <td>0.020735</td>\n",
       "      <td>-0.055586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reducestress</th>\n",
       "      <td>0.140034</td>\n",
       "      <td>0.413750</td>\n",
       "      <td>0.065712</td>\n",
       "      <td>0.013251</td>\n",
       "      <td>0.091322</td>\n",
       "      <td>0.028433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0         1         2         3         4         5\n",
       "everbev          0.179662  0.145763  0.887842  0.192520  0.103690  0.142331\n",
       "everherb         0.119083  0.139673  0.226104  0.249323  0.063934  0.062603\n",
       "evermed          0.075596  0.025610  0.121930  0.082592  0.038293  0.054431\n",
       "oats             0.298654  0.216706  0.590053  0.310725  0.133938  0.140622\n",
       "lacttreathome    0.154380  0.109423  0.253263  0.638298  0.150215  0.049424\n",
       "lacttreatstore   0.115899  0.172955  0.459063  0.075077  0.177784  0.007863\n",
       "brewersyeast     0.145918  0.102172  0.172322  0.771507  0.150839  0.031305\n",
       "coconutdrink     0.208517  0.090722  0.222441  0.218198  0.534585  0.124797\n",
       "sportdrink       0.204915  0.215248  0.488642  0.134079  0.379780  0.103060\n",
       "pinkdrink       -0.026280  0.062784  0.151990  0.130657  0.542318  0.025780\n",
       "noalcbeer        0.135922 -0.012549  0.056632  0.051132  0.006788  0.252401\n",
       "beer             0.058086  0.068677  0.112334  0.091449  0.100682  0.618432\n",
       "wine             0.167479  0.097125  0.036953 -0.031013  0.008981  0.499941\n",
       "lacttea          0.153066  0.108205  0.404702  0.091486  0.020758  0.039712\n",
       "fruit            0.750962  0.167580  0.252348  0.072081  0.015973  0.143569\n",
       "veg              0.811992  0.146391  0.200592  0.146618  0.019845  0.089903\n",
       "seeds            0.343782  0.169475  0.163658  0.674930  0.076153  0.077445\n",
       "beans            0.692657  0.144863  0.062666  0.144969  0.047766  0.193123\n",
       "wholegrains      0.614104  0.100098  0.146407  0.171936  0.107023  0.070639\n",
       "nuts             0.677339  0.153917  0.175417  0.250045  0.070032  0.115522\n",
       "meat             0.753136  0.172618  0.209789  0.060432  0.057661  0.172993\n",
       "increasebf       0.169730  0.506908  0.128061  0.022067  0.160797  0.033705\n",
       "increaseexpress  0.084712  0.670364  0.050757  0.078129  0.023845  0.057424\n",
       "skintoskin       0.174477  0.410895  0.057244 -0.029032  0.204508  0.013371\n",
       "shield          -0.039688  0.434484  0.094826  0.061327 -0.017211  0.015939\n",
       "massage          0.046436  0.678201  0.081162  0.088476  0.015768  0.039575\n",
       "pumpwhilebf      0.091966  0.465718  0.025837  0.041602  0.033764  0.053686\n",
       "pumpboth         0.025432  0.659752  0.163565  0.091394 -0.070359  0.058227\n",
       "pumpafter        0.112772  0.617221  0.086803  0.047397  0.018638  0.076224\n",
       "warmed           0.087590  0.426476  0.093045  0.130618  0.020735 -0.055586\n",
       "reducestress     0.140034  0.413750  0.065712  0.013251  0.091322  0.028433"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor_df=pd.DataFrame(fa.loadings_,index=questionaire_df.columns)\n",
    "factor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.70303749, 3.29805757, 2.31592903, 1.9614138 , 0.95839826,\n",
       "        0.91760569]),\n",
       " array([0.11945282, 0.10638895, 0.07470739, 0.06327141, 0.03091607,\n",
       "        0.02960018]),\n",
       " array([0.11945282, 0.22584178, 0.30054916, 0.36382058, 0.39473665,\n",
       "        0.42433683]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get variance of each factors\n",
    "fa.get_factor_variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total 42% cumulative Variance explained by the 6 factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referrence: https://www.datacamp.com/tutorial/introduction-factor-analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
